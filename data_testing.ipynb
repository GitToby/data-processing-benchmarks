{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_magics\n",
    "# imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/toby.devlin/dev/projects/data-processing/data/Crime_Data_from_2020_to_Present.csv\n",
      "/Users/toby.devlin/dev/projects/data-processing/data/reddit_account_data.csv\n"
     ]
    }
   ],
   "source": [
    "# consts\n",
    "_data_root = Path(os.sep.join([\"/\", \"Users\", \"toby.devlin\", \"dev\", \"projects\", \"data-processing\", \"data\"]))\n",
    "crime_path = (_data_root / \"Crime_Data_from_2020_to_Present.csv\").resolve()\n",
    "reddit_path = (_data_root / \"reddit_account_data.csv\").resolve()\n",
    "print(crime_path)\n",
    "print(reddit_path)\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we are doing\n",
    "We have 3 files, as printed above.\n",
    "These have Various schemas & we want to do some processing in various tools to test performance. The tests themselves make no sense but the data is large enough and  \"real\" enough to be somewhat reflective of a real world scenario without any need to adjust for random generators.\n",
    "\n",
    "## Data Sourcing\n",
    "- [Crime_Data_from_2020_to_Present.csv](Crime_Data_from_2020_to_Present.csv) -> https://catalog.data.gov/dataset/crime-data-from-2020-to-present\n",
    "- [reddit_account_data.csv](reddit_account_data.csv) -> https://files.pushshift.io/reddit/\n",
    "\n",
    "## The Test:\n",
    "1. Read in `Crime_Data_from_2020_to_Present.csv` to a frame, converting all datetime to native datetime format: `Date Rptd`, `DATE OCC`\n",
    "2. Filter this frame such that:\n",
    "   1. Column `Vict Age` > 15\n",
    "   2. Use only records in the quantiles 0.05 -> 0.95 for `LAT` and `LON`\n",
    "3. Read in the `reddit_account_data.csv` to a frame, converting all datetime to native datetime format: `created_utc`, `updated_on`\n",
    "4. Filter this frame such that:\n",
    "   1. None of the account names end with the number 7\n",
    "5. Create a column in the `account_data` frame, and assign each of the users a `group` value, uniformly, from the `crime_data.DR_NO` column\n",
    "6. Join these two frames together on `account_data.group == crime_data.DR_NO`\n",
    "7. Select only those who are NOT part of the following groups:\n",
    "    1. `CC`\n",
    "    2. `AA`\n",
    "8. Group these by the first 2 letters of their name and calculate:\n",
    "   1. The average time since reporting the crime\n",
    "   2. The number of members in the group\n",
    "   3. The user with the highest & lowest Karma scores\n",
    "   4. The average time since creating an account\n",
    "\n",
    "## What we are monitoring\n",
    "- Time taken, using the timeit module\n",
    "- Memory usage, using [rusmux's ipython-memory-magics](https://github.com/rusmux/ipython-memory-magics), which should just look at the cells memory usage.\n",
    "\n",
    "# Before we start\n",
    "The files themselves should be looked at first; their size, shape and location on disk. todo: also stream from s3\n",
    "\n",
    "| file                                |                       size |    records | columns |\n",
    "|-------------------------------------|---------------------------:|-----------:|:--------|\n",
    "| Crime_Data_from_2020_to_Present.csv |   168,553,714 bytes (161M) |    659,640 | 28      |\n",
    "| reddit_account_data.csv             | 3,298,086,717 bytes (3.1G) | 69,382,538 | 6       |\n",
    "\n",
    "The both these frames are relatively small but we are hoping to show there is some extra on disk processing we can do to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
