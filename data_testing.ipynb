{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext memory_magics\n",
    "# imports\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/toby.devlin/dev/projects/data-processing/data/Crime_Data_from_2020_to_Present.csv\n",
      "/Users/toby.devlin/dev/projects/data-processing/data/reddit_account_data.csv\n"
     ]
    }
   ],
   "source": [
    "# consts\n",
    "_data_root = Path(os.sep.join([\"/\", \"Users\", \"toby.devlin\", \"dev\", \"projects\", \"data-processing\", \"data\"]))\n",
    "crime_path = (_data_root / \"Crime_Data_from_2020_to_Present.csv\").resolve()\n",
    "reddit_path = (_data_root / \"reddit_account_data.csv\").resolve()\n",
    "print(crime_path)\n",
    "print(reddit_path)\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we are doing\n",
    "We have 3 files, as printed above.\n",
    "These have Various schemas & we want to do some processing in various tools to test performance. The tests themselves make no sense but the data is large enough and  \"real\" enough to be somewhat reflective of a real world scenario without any need to adjust for random generators.\n",
    "\n",
    "## Data Sourcing\n",
    "- [Crime_Data_from_2020_to_Present.csv](Crime_Data_from_2020_to_Present.csv) -> https://catalog.data.gov/dataset/crime-data-from-2020-to-present\n",
    "- [reddit_account_data.csv](reddit_account_data.csv) -> https://files.pushshift.io/reddit/\n",
    "\n",
    "## The Test:\n",
    "1. Read in `Crime_Data_from_2020_to_Present.csv` to a frame, converting all datetime to native datetime format: `Date Rptd`, `DATE OCC`\n",
    "2. Filter this frame such that:\n",
    "   1. Column `Vict Age` > 15\n",
    "   2. Use only records in the quantiles 0.05 -> 0.95 for `LAT` and `LON`\n",
    "3. Read in the `reddit_account_data.csv` to a frame, converting all datetime to native datetime format: `created_utc`, `updated_on`\n",
    "4. Filter this frame such that:\n",
    "   1. None of the account names end with the number 7\n",
    "5. Create a column in the `account_data` frame, and assign each of the users a `group` value, uniformly, from the `crime_data.DR_NO` column\n",
    "6. Join these two frames together on `account_data.group == crime_data.DR_NO`\n",
    "7. Select only those who are NOT part of the following groups:\n",
    "    1. `CC`\n",
    "    2. `AA`\n",
    "8. Group these by the first 2 letters of their name and calculate:\n",
    "   1. The average time since reporting the crime\n",
    "   2. The number of members in the group\n",
    "   3. The user with the highest & lowest Karma scores\n",
    "   4. The average time since creating an account\n",
    "\n",
    "## What we are monitoring\n",
    "- Time taken, using the timeit module\n",
    "- Memory usage, using [rusmux's ipython-memory-magics](https://github.com/rusmux/ipython-memory-magics), which should just look at the cells memory usage.\n",
    "\n",
    "# Before we start\n",
    "The files themselves should be looked at first; their size, shape and location on disk. todo: also stream from s3\n",
    "\n",
    "| file                                |                       size |    records | columns |\n",
    "|-------------------------------------|---------------------------:|-----------:|:--------|\n",
    "| Crime_Data_from_2020_to_Present.csv |   168,553,714 bytes (161M) |    659,640 | 28      |\n",
    "| reddit_account_data.csv             | 3,298,086,717 bytes (3.1G) | 69,382,538 | 6       |\n",
    "\n",
    "The both these frames are relatively small, but we are hoping to show there is some extra on disk processing we can do to speed things up, for example polars will push down predicates and scan filters on disk.\n",
    "\n",
    "# The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "log_polars = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def do_polars_test():\n",
    "    log_polars.info(f\"using file crime_path: {crime_path}\")\n",
    "    log_polars.info(f\"using file reddit_path: {reddit_path}\")\n",
    "    # 1. Read in `Crime_Data_from_2020_to_Present.csv` to a frame, converting all datetime to native datetime format: `Date Rptd`, `DATE OCC`\n",
    "    df_crime = pl.scan_csv(crime_path, try_parse_dates=True)\n",
    "    df_reddit = pl.scan_csv(reddit_path, try_parse_dates=True)\n",
    "\n",
    "    distinct_ids = df_crime.select([pl.col(\"DR_NO\").unique()]).collect().to_series()\n",
    "\n",
    "    q0 = (\n",
    "        # 3. Read in the `reddit_account_data.csv` to a frame, converting all datetime to native datetime format: `created_utc`, `updated_on`\n",
    "        df_reddit\n",
    "        # 4. Filter this frame such that:\n",
    "        #    1. None of the account names end with a number\n",
    "        .filter(~pl.col(\"name\").str.contains(\"abc\")).filter(~pl.col(\"name\").str.ends_with(\"7\"))\n",
    "        # 5. Create a column in the `account_data` frame, and assign each of the users a `group` value, uniformly, from the `crime_data.DR_NO` column\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"name\").apply(lambda _: distinct_ids.sample(seed=0, with_replacement=True)[0]).alias(\"DR_NO\"),\n",
    "                pl.from_epoch(pl.col(\"created_utc\"), unit=\"s\").alias(\"created_utc\"),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    q1 = (\n",
    "        df_crime\n",
    "        # 2. Filter this frame such that:\n",
    "        #    1. Column `Vict Age` > 15\n",
    "        .filter(pl.col(\"Vict Age\") > 15)\n",
    "        #    2. Use only records in the quantiles 0.05 -> 0.95 for `LAT` and `LON`\n",
    "        .filter(pl.col(\"LAT\").quantile(0.05) < pl.col(\"LAT\"))\n",
    "        .filter(pl.col(\"LAT\").quantile(0.95) > pl.col(\"LAT\"))\n",
    "        .filter(pl.col(\"LON\").quantile(0.05) < pl.col(\"LON\"))\n",
    "        .filter(pl.col(\"LON\").quantile(0.95) > pl.col(\"LON\"))\n",
    "        # 6. Join these two frames together on `account_data.Group == crime_data.Status`\n",
    "        .join(q0, on=\"DR_NO\")\n",
    "        # 7. Select only those who are NOT part of the following groups:\n",
    "        #     1. `CC`\n",
    "        #     2. `AA`\n",
    "        .filter(~pl.col(\"Status\").is_in((\"CC\", \"AA\")))\n",
    "        # 8. Group these by the lower case first letter of their name and calculate:\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"name\").apply(lambda name: name[0].lower()).alias(\"grp_by\"),\n",
    "                (pl.col(\"comment_karma\") + pl.col(\"link_karma\")).alias(\"karma_sum\"),\n",
    "            ]\n",
    "        )\n",
    "        .groupby(\"grp_by\")\n",
    "        .agg(\n",
    "            [\n",
    "                #    1. The average time since reporting the crime as `mean_time_since_reports`\n",
    "                pl.col(\"Date Rptd\")\n",
    "                .drop_nulls()\n",
    "                .apply(lambda x: x - datetime.now())\n",
    "                .mean()\n",
    "                .alias(\"mean_time_since_reports\"),\n",
    "                #    2. The number of members in the group as `count_members`\n",
    "                pl.col(\"name\").count().alias(\"count_members\"),\n",
    "                #    3. The user with the highest & lowest Karma scores as `highest_karma` and `lowest_karma`\n",
    "                pl.col(\"karma_sum\").max().alias(\"highest_karma\"),\n",
    "                pl.col(\"karma_sum\").min().alias(\"lowest_karma\"),\n",
    "                #    4. The average time since creating an account as `mean_time_since_account_open`\n",
    "                pl.col(\"created_utc\")\n",
    "                .drop_nulls()\n",
    "                .apply(lambda x: x - datetime.now())\n",
    "                .mean()\n",
    "                .alias(\"mean_time_since_account_open\"),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return q1.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_processing.const import crime_path, reddit_path\n",
    "\n",
    "log_pandas = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def do_pandas_test():\n",
    "    log_pandas.info(f\"using file crime_path: {crime_path}\")\n",
    "    log_pandas.info(f\"using file reddit_path: {reddit_path}\")\n",
    "    # Pandas\n",
    "    # 1. Read in `Crime_Data_from_2020_to_Present.csv` to a frame, converting all datetime to native datetime format: `Date Rptd`, `DATE OCC`\n",
    "    df_crime = pd.read_csv(\n",
    "        crime_path, parse_dates=[\"Date Rptd\", \"DATE OCC\"], dtype={\"Vict Age\": int, \"LAT\": float, \"LON\": float}\n",
    "    )\n",
    "\n",
    "    # 2. Filter this frame such that:\n",
    "    #    1. Column `Vict Age` > 15\n",
    "    #    2. Use only records in the quantiles 0.05 -> 0.95 for `LAT` and `LON`\n",
    "    df_crime = df_crime[df_crime[\"Vict Age\"] > 15]\n",
    "    df_crime = df_crime[df_crime[\"LAT\"].quantile(0.05) < df_crime[\"LAT\"]]\n",
    "    df_crime = df_crime[df_crime[\"LAT\"] < df_crime[\"LAT\"].quantile(0.95)]\n",
    "    df_crime = df_crime[df_crime[\"LON\"].quantile(0.05) < df_crime[\"LON\"]]\n",
    "    df_crime = df_crime[df_crime[\"LON\"] < df_crime[\"LON\"].quantile(0.95)]\n",
    "\n",
    "    # 3. Read in the `reddit_account_data.csv` to a frame, converting all datetime to native datetime format: `created_utc`, `updated_on`\n",
    "    df_reddit = pd.read_csv(reddit_path)\n",
    "    df_reddit[\"created_utc\"] = pd.to_datetime(df_reddit[\"created_utc\"], unit=\"s\")\n",
    "    df_reddit[\"updated_on\"] = pd.to_datetime(df_reddit[\"updated_on\"], unit=\"s\")\n",
    "\n",
    "    # 4. Filter this frame such that:\n",
    "    #    1. None of the account names end in the number 7\n",
    "    #    2. None of the account names have the string abc in them\n",
    "    df_reddit = df_reddit[df_reddit[\"name\"].str.contains(\"abc\") == False]\n",
    "    df_reddit = df_reddit[df_reddit[\"name\"].str.endswith(\"7\") == False]\n",
    "    # df_reddit = df_reddit[df_reddit[\"name\"].str.match(\"(.*abc.*|.*7$)\") == False]\n",
    "\n",
    "    # 5. Create a column in the `account_data` frame, and assign each of the users a `group` value, uniformly, from the `crime_data.DR_NO` column\n",
    "    rng = np.random.default_rng()\n",
    "    unique = df_crime[\"DR_NO\"].unique()\n",
    "    df_reddit[\"group\"] = rng.choice(unique, len(df_reddit))\n",
    "\n",
    "    # 6. Join these two frames together on `account_data.group == crime_data.DR_NO`\n",
    "    df_merge = df_reddit.merge(df_crime, how=\"inner\", right_on=\"DR_NO\", left_on=\"group\")\n",
    "\n",
    "    # 7. Select only those who are NOT part of the following Statuses:\n",
    "    #     1. `CC`\n",
    "    #     2. `AA`\n",
    "    df_merge = df_merge[~df_merge[\"Status\"].isin((\"CC\", \"AA\"))]\n",
    "\n",
    "    df_merge[\"grp_by\"] = [s[0].lower() for s in df_merge[\"name\"]]\n",
    "    df_merge[\"karma_sum\"] = df_merge[\"comment_karma\"] + df_merge[\"link_karma\"]\n",
    "    df_merge[\"since_rptd\"] = df_merge[\"Date Rptd\"] - pd.Timestamp.now()\n",
    "    df_merge[\"since_open\"] = df_merge[\"created_utc\"] - pd.Timestamp.now()\n",
    "    groups = df_merge.groupby(\"grp_by\")\n",
    "\n",
    "    # 8. Group these by the lower case first letter of their name and calculate:\n",
    "    res = groups.agg(\n",
    "        #    1. The average time since reporting the crime as `mean_time_since_reports`\n",
    "        mean_time_since_reports=pd.NamedAgg(column=\"since_rptd\", aggfunc=np.mean),\n",
    "        #    2. The number of members in the group as `count_members`\n",
    "        count_members=pd.NamedAgg(column=\"id\", aggfunc=\"count\"),\n",
    "        #    3. The user with the highest & lowest Karma scores as `highest_karma` and `lowest_karma`\n",
    "        highest_karma=pd.NamedAgg(column=\"karma_sum\", aggfunc=np.max),\n",
    "        lowest_karma=pd.NamedAgg(column=\"karma_sum\", aggfunc=np.min),\n",
    "        #    4. The average time since creating an account as `mean_time_since_account_open`\n",
    "        mean_time_since_account_open=pd.NamedAgg(column=\"since_open\", aggfunc=np.mean),\n",
    "    )\n",
    "    return res\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code is pretty well explained, so next up is how we run it: Unfortunately I was [having trouble](https://github.com/actions/setup-python/issues/350) with running the code using a GitHub runner. Below is a very simple benchmark harness that i will try using."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "\n",
    "from data_processing.const import out_path\n",
    "from data_processing.pandas import do_pandas_test\n",
    "from data_processing.polars import do_polars_test\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)8s - %(message)s\")\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def do_test(func: Callable):\n",
    "    n = 5\n",
    "    run_times = dict()\n",
    "    log.info(f\"start {func.__name__}\")\n",
    "    for i in range(n):\n",
    "        counter_start = time.perf_counter_ns()\n",
    "        func()\n",
    "        counter_end = time.perf_counter_ns() - counter_start\n",
    "        log.info(f\"<<< Ending {func.__name__} round {i} after {counter_end}ns\")\n",
    "        run_times[n] = {\"ns taken\": counter_end - counter_start}\n",
    "\n",
    "    with open(out_path / f\"{func.__name__}.{datetime.now().isoformat()}.json\", \"w+\") as f:\n",
    "        json.dump(run_times, f)\n",
    "\n",
    "\n",
    "do_test(do_polars_test)\n",
    "do_test(do_pandas_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This spat out the following results:\n",
    "\n",
    "```bash\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
